# -*- coding: utf-8 -*-
"""0D復習_三層NNWの実装.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gkNNiSK7qnB9k2cOHgxekRIgdtdBob0C

# **三層ニューラルネットワークの実装**
"""

import numpy as np

"""# **入力層**"""

X = np.array([1.0, 0.5])#入力(x1, x2)
W1 = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]])#重み(w11, w12, w21, w22, w31, w32)
B1 = np.array([0.1, 0.2, 0.3])#バイアスb1
W2 = np.array([[0.1, 0.4], [0.2, 0.5],[0.3, 0.6]])#重み(w11, w12, w21, w22, w31, w32)
B2 = np.array([0.1, 0.2])#バイアスb2
W3 = np.array([[0.1, 0.3], [0.2, 0.4]])#重み(w11, w12, w21, w22, w31, w32)
B3 = np.array([0.1, 0.2])#バイアスb3

"""# **活性化関数①：シグモイド関数**"""

def sigmoid(x):
  return 1 / (1 + np.exp(-x))

"""# **活性化関数②：Relu関数**"""

def  relu(x):
  return np.maximum(0, x)

"""# **活性化関数③：ソフトマックス関数**"""

def softmax(x):
  c = np.max(x)
  exp_a = np.exp(x - c)
  sum_exp_a = np.sum(exp_a)
  y = exp_a / sum_exp_a

  return y

"""# **中間層-第一層**"""

A1 = np.dot(X, W1) + B1
print(A1)

Z1 = sigmoid(A1)
print(Z1)

Z1 = relu(A1)
print(Z1)

print(Z1)

"""# **中間層ー第二層**"""

A2 = np.dot(Z1, W2) + B2
Z2 = sigmoid(A2)
print(Z2)

"""# 出力層"""

def identity_function(x):
  return x

A3 = np.dot(Z2, W3) + B3
Y = identity_function(A3)
print(Y)

"""# 実装のまとめ"""

def init_network():#重みとバイアスの初期化
  network = {}
  network['W1'] = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]])
  network['b1'] = np.array([0.1, 0.2, 0.3])
  network['W2'] = np.array([[0.1, 0.4], [0.2, 0.5],[0.3, 0.6]])
  network['b2'] = np.array([0.1, 0.2])
  network['W3'] = np.array([[0.1, 0.3], [0.2, 0.4]])
  network['b3'] = np.array([0.1, 0.2])

  return network

def foward (network, x):
  W1, W2, W3 = network['W1'], network['W2'], network['W3']
  b1, b2, b3 = network['b1'], network['b2'], network['b3']

  a1 = np.dot(x, W1) + b1 
  z1 = sigmoid(a1)

  A2 = np.dot(Z1, W2) + B2
  Z2 = sigmoid(A2)

  A3 = np.dot(Z2, W3) + B3
  y = identity_function(A3)

  return y

network = init_network()
x = np.array([1.0, 0.5])
y = foward(network, x)
print(y)

